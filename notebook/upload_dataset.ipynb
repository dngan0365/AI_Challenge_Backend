{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374b56de",
   "metadata": {},
   "source": [
    "# Download, Upload, and Unzip Large Datasets to Google Cloud Storage\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Download large .zip files from URLs and upload them directly to Google Cloud Storage (GCS) without filling up local disk.\n",
    "2. Unzip the files inside a Google Cloud VM (or Vertex AI Workbench) and upload the extracted files back to GCS.\n",
    "\n",
    "## Steps\n",
    "\n",
    "- **Step 1:** Download and upload .zip files to GCS.\n",
    "- **Step 2:** Unzip files inside a GCP VM and upload extracted files back to GCS.\n",
    "- **Step 3:** (Optional) Automate the unzip and upload process in Python.\n",
    "\n",
    "> **Note:** Do not unzip in Colab if your dataset is very large. Use a VM or Vertex AI Workbench for unzipping and uploading extracted files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88abb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "key_json_path = \"../service-account.json\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(key_json_path)\n",
    "\n",
    "storage_client = storage.Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45989909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize GCS client\u001b[39;00m\n\u001b[32m      6\u001b[39m bucket_name = \u001b[33m\"\u001b[39m\u001b[33mtest-video-retrieval\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m bucket = \u001b[43mstorage_client\u001b[49m.bucket(bucket_name)\n",
      "\u001b[31mNameError\u001b[39m: name 'storage_client' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize GCS client\n",
    "bucket_name = \"test-video-retrieval\"\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f149439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# List of files and URLs\n",
    "files = {\n",
    "    \"Keyframes_L21.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L21.zip\",\n",
    "    \"Keyframes_L22.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L22.zip\",\n",
    "    \"Keyframes_L23.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L23.zip\",\n",
    "    \"Keyframes_L24.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L24.zip\",\n",
    "    \"Keyframes_L25.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L25.zip\",\n",
    "    \"Keyframes_L26_a.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L26_a.zip\",\n",
    "    \"Keyframes_L26_b.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L26_b.zip\",\n",
    "    \"Keyframes_L26_c.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L26_c.zip\",\n",
    "    \"Keyframes_L26_d.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L26_d.zip\",\n",
    "    \"Keyframes_L26_e.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L26_e.zip\",\n",
    "    \"Keyframes_L27.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L27.zip\",\n",
    "    \"Keyframes_L28.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L28.zip\",\n",
    "    \"Keyframes_L29.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L29.zip\",\n",
    "    \"Keyframes_L30.zip\": \"https://aic-data.ledo.io.vn/Keyframes_L30.zip\",\n",
    "    \"Videos_L21_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L21_a.zip\",\n",
    "    \"Videos_L22_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L22_a.zip\",\n",
    "    \"Videos_L23_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L23_a.zip\",\n",
    "    \"Videos_L24_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L24_a.zip\",\n",
    "    \"Videos_L25_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L25_a.zip\",\n",
    "    \"Videos_L25_a1.zip\": \"https://aic-data.ledo.io.vn/Videos_L25_a1.zip\",\n",
    "    \"Videos_L25_b.zip\": \"https://aic-data.ledo.io.vn/Videos_L25_b.zip\",\n",
    "    \"Videos_L26_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L26_a.zip\",\n",
    "    \"Videos_L26_b.zip\": \"https://aic-data.ledo.io.vn/Videos_L26_b.zip\",\n",
    "    \"Videos_L26_c.zip\": \"https://aic-data.ledo.io.vn/Videos_L26_c.zip\",\n",
    "    \"Videos_L26_d.zip\": \"https://aic-data.ledo.io.vn/Videos_L26_d.zip\",\n",
    "    \"Videos_L26_e.zip\": \"https://aic-data.ledo.io.vn/Videos_L26_e.zip\",\n",
    "    \"Videos_L27_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L27_a.zip\",\n",
    "    \"Videos_L28_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L28_a.zip\",\n",
    "    \"Videos_L29_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L29_a.zip\",\n",
    "    \"Videos_L30_a.zip\": \"https://aic-data.ledo.io.vn/Videos_L30_a.zip\",\n",
    "    \"clip-features-32-aic25-b1.zip\": \"https://aic-data.ledo.io.vn/clip-features-32-aic25-b1.zip\",\n",
    "    \"map-keyframes-aic25-b1.zip\": \"https://aic-data.ledo.io.vn/map-keyframes-aic25-b1.zip\",\n",
    "    \"media-info-aic25-b1.zip\": \"https://aic-data.ledo.io.vn/media-info-aic25-b1.zip\",\n",
    "    \"objects-aic25-b1.zip\": \"https://aic-data.ledo.io.vn/objects-aic25-b1.zip\"\n",
    "}\n",
    "\n",
    "def download_and_upload(filename, url):\n",
    "    local_path = f\"{filename}\"\n",
    "    # Stream download to avoid memory issues\n",
    "    with requests.get(url, stream=True, timeout=None) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):  # 1 MB\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"Downloaded {filename} to {local_path}\")\n",
    "\n",
    "    # Upload to GCS\n",
    "    blob = bucket.blob(f\"dataset/raw_zips/{filename}\")\n",
    "    blob.chunk_size = 5 * 1024 * 1024\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded {filename} to gs://{bucket_name}/raw_zips/\")\n",
    "    os.remove(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367357c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframes_L21.zip: 1.35 GB\n",
      "Keyframes_L22.zip: 1.60 GB\n",
      "Keyframes_L23.zip: 0.47 GB\n",
      "Keyframes_L24.zip: 1.61 GB\n",
      "Keyframes_L25.zip: 5.67 GB\n",
      "Keyframes_L26_a.zip: 2.14 GB\n",
      "Keyframes_L26_b.zip: 2.25 GB\n",
      "Keyframes_L26_c.zip: 2.31 GB\n",
      "Keyframes_L26_d.zip: 2.30 GB\n",
      "Keyframes_L26_e.zip: 2.30 GB\n",
      "Keyframes_L27.zip: 1.02 GB\n",
      "Keyframes_L28.zip: 2.02 GB\n",
      "Keyframes_L29.zip: 2.34 GB\n",
      "Keyframes_L30.zip: 1.31 GB\n",
      "Videos_L21_a.zip: 3.15 GB\n",
      "Videos_L22_a.zip: 3.87 GB\n",
      "Videos_L23_a.zip: 1.90 GB\n",
      "Videos_L24_a.zip: 5.40 GB\n",
      "Videos_L25_a.zip: 11.97 GB\n",
      "Videos_L25_a1.zip: 6.72 GB\n",
      "Videos_L25_b.zip: 5.13 GB\n",
      "Videos_L26_a.zip: 6.13 GB\n",
      "Videos_L26_b.zip: 6.37 GB\n",
      "Videos_L26_c.zip: 6.43 GB\n",
      "Videos_L26_d.zip: 6.31 GB\n",
      "Videos_L26_e.zip: 6.46 GB\n",
      "Videos_L27_a.zip: 2.37 GB\n",
      "Videos_L28_a.zip: 6.77 GB\n",
      "Videos_L29_a.zip: 6.30 GB\n",
      "Videos_L30_a.zip: 3.85 GB\n",
      "clip-features-32-aic25-b1.zip: 0.16 GB\n",
      "map-keyframes-aic25-b1.zip: 0.00 GB\n",
      "media-info-aic25-b1.zip: 0.00 GB\n",
      "objects-aic25-b1.zip: 0.60 GB\n",
      "Total storage: 118.58 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = 0\n",
    "\n",
    "for name, url in files.items():\n",
    "    response = requests.head(url)\n",
    "    size = int(response.headers.get('Content-Length', 0))\n",
    "    total_size_bytes += size\n",
    "    print(f\"{name}: {size / (1024**3):.2f} GB\")  # size in GB\n",
    "\n",
    "print(f\"Total storage: {total_size_bytes / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab4169f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(5194799680 bytes read, 897715842 more expected)', IncompleteRead(5194799680 bytes read, 897715842 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIncompleteRead\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:779\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    782\u001b[39m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:925\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    916\u001b[39m         \u001b[38;5;28mself\u001b[39m.enforce_content_length\n\u001b[32m    917\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    923\u001b[39m         \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[32m    924\u001b[39m         \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m._fp_bytes_read, \u001b[38;5;28mself\u001b[39m.length_remaining)\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read1 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    927\u001b[39m     (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length_remaining == \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m    928\u001b[39m ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001b[39;00m\n\u001b[32m    932\u001b[39m     \u001b[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001b[39;00m\n",
      "\u001b[31mIncompleteRead\u001b[39m: IncompleteRead(5194799680 bytes read, 897715842 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:1008\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) < amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[32m   1007\u001b[39m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m     decoded_data = \u001b[38;5;28mself\u001b[39m._decode(data, decode_content, flush_decoder)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:903\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\urllib3\\response.py:803\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    802\u001b[39m         arg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(arg, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection broken: IncompleteRead(5194799680 bytes read, 897715842 more expected)', IncompleteRead(5194799680 bytes read, 897715842 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mChunkedEncodingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, url \u001b[38;5;129;01min\u001b[39;00m files.items():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdownload_and_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mdownload_and_upload\u001b[39m\u001b[34m(filename, url)\u001b[39m\n\u001b[32m     43\u001b[39m r.raise_for_status()\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m*\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 1 MB\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mt200\\OneDrive\\Desktop\\AI\\AI_challenge\\software\\back-end\\.venv-nb\\Lib\\site-packages\\requests\\models.py:822\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[31mChunkedEncodingError\u001b[39m: ('Connection broken: IncompleteRead(5194799680 bytes read, 897715842 more expected)', IncompleteRead(5194799680 bytes read, 897715842 more expected))"
     ]
    }
   ],
   "source": [
    "for name, url in files.items():\n",
    "    download_and_upload(name, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8c1d6",
   "metadata": {},
   "source": [
    "## Step 2: Unzip and Upload Extracted Files Back to GCS on a GCP VM\n",
    "\n",
    "After uploading the .zip files to GCS, use a Compute Engine VM (or Vertex AI Workbench) to unzip and upload the extracted files:\n",
    "\n",
    "1. **Install required tools:**\n",
    "   ```sh\n",
    "   sudo apt-get update && sudo apt-get install unzip -y\n",
    "   ```\n",
    "2. **Copy a zip file from GCS:**\n",
    "   ```sh\n",
    "   gsutil cp gs://your-bucket/raw_zips/Keyframes_L21.zip .\n",
    "   ```\n",
    "3. **Unzip locally:**\n",
    "   ```sh\n",
    "   unzip Keyframes_L21.zip -d Keyframes_L21/\n",
    "   ```\n",
    "4. **Upload extracted files back to GCS:**\n",
    "   ```sh\n",
    "   gsutil -m cp -r Keyframes_L21/ gs://your-bucket/unzipped/\n",
    "   ```\n",
    "\n",
    "Repeat for each zip file as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# This cell should be run on a GCP VM or Vertex AI Workbench with access to the bucket.\n",
    "# It will download each zip from GCS, unzip it, and upload the extracted files back to GCS.\n",
    "\n",
    "# Initialize GCS client\n",
    "bucket_name = \"test-video-retrieval\"\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# List all blobs in the raw_zips folder\n",
    "blobs = bucket.list_blobs(prefix=\"dataset/raw_zips/\")\n",
    "\n",
    "for blob in blobs:\n",
    "    filename = os.path.basename(blob.name)\n",
    "    local_zip = f\"/{filename}\"\n",
    "    print(f\"Processing {filename}\")\n",
    "\n",
    "    # Download zip from GCS\n",
    "    blob.download_to_filename(local_zip)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "    # Unzip\n",
    "    extract_dir = f\"/{filename}\"\n",
    "    with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Unzipped {filename}\")\n",
    "\n",
    "    # Upload extracted files back to GCS\n",
    "    for root, dirs, files in os.walk(extract_dir):\n",
    "        for f in files:\n",
    "            local_path = os.path.join(root, f)\n",
    "            rel_path = os.path.relpath(local_path, extract_dir)\n",
    "            new_blob = bucket.blob(f\"unzipped/{filename}/{rel_path}\")\n",
    "            new_blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded unzipped {filename} to GCS\")\n",
    "\n",
    "    os.remove(local_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f796c",
   "metadata": {},
   "source": [
    "## Tips and Troubleshooting\n",
    "\n",
    "- For very large files, consider increasing the VM disk size or using a VM with SSD.\n",
    "- Use `gsutil -m` for parallel uploads to speed up transfers.\n",
    "- If you have many files, you can batch process them by modifying the Python code to process a subset at a time.\n",
    "- If you run into permission errors, make sure your VM service account has Storage Object Admin permissions.\n",
    "- Clean up `/tmp` after processing to avoid running out of disk space.\n",
    "\n",
    "---\n",
    "\n",
    "You can now efficiently move and extract large datasets between public URLs and Google Cloud Storage!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
