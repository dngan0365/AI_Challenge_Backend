{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55482415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import whisper\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import pytesseract\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class VideoMetadata:\n",
    "    video_id: str\n",
    "    filename: str\n",
    "    duration: float\n",
    "    fps: float\n",
    "    resolution: Tuple[int, int]\n",
    "    location: str = \"\"\n",
    "    topic: str = \"\"\n",
    "    cluster_id: int = -1\n",
    "\n",
    "@dataclass\n",
    "class KeyframeData:\n",
    "    frame_id: str\n",
    "    video_id: str\n",
    "    timestamp: float\n",
    "    image_path: str\n",
    "    caption: str\n",
    "    objects: List[Dict]\n",
    "    poses: List[Dict]\n",
    "    actions: List[str]\n",
    "    ocr_text: str\n",
    "    scene_embedding: np.ndarray\n",
    "    text_embedding: np.ndarray\n",
    "    pose_embedding: np.ndarray\n",
    "\n",
    "class GoogleStorageManager:\n",
    "    def __init__(self, bucket_name: str, credentials_path: str = None):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.client = storage.Client.from_service_account_json(credentials_path) if credentials_path else storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "    \n",
    "    def upload_file(self, local_path: str, remote_path: str):\n",
    "        \"\"\"Upload file to Google Storage\"\"\"\n",
    "        blob = self.bucket.blob(remote_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        return f\"gs://{self.bucket_name}/{remote_path}\"\n",
    "    \n",
    "    def download_file(self, remote_path: str, local_path: str):\n",
    "        \"\"\"Download file from Google Storage\"\"\"\n",
    "        blob = self.bucket.blob(remote_path)\n",
    "        blob.download_to_filename(local_path)\n",
    "    \n",
    "    def list_files(self, prefix: str = \"\") -> List[str]:\n",
    "        \"\"\"List files with given prefix\"\"\"\n",
    "        blobs = self.bucket.list_blobs(prefix=prefix)\n",
    "        return [blob.name for blob in blobs]\n",
    "\n",
    "class MultiModalAnalyzer:\n",
    "    def __init__(self, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all required models\"\"\"\n",
    "        logger.info(\"Loading models...\")\n",
    "        \n",
    "        # Image captioning\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(self.device)\n",
    "        \n",
    "        # CLIP for embeddings\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        \n",
    "        # Object detection\n",
    "        self.yolo_model = YOLO('yolov8n.pt')\n",
    "        \n",
    "        # Pose estimation\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=True,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=True,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Speech recognition\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        logger.info(\"All models loaded successfully!\")\n",
    "    \n",
    "    def extract_keyframes(self, video_path: str, interval: int = 30) -> List[Tuple[float, np.ndarray]]:\n",
    "        \"\"\"Extract keyframes at regular intervals\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        keyframes = []\n",
    "        \n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % (fps * interval) == 0:  # Every 'interval' seconds\n",
    "                timestamp = frame_count / fps\n",
    "                keyframes.append((timestamp, frame))\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        return keyframes\n",
    "    \n",
    "    def generate_caption(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Generate image caption using BLIP\"\"\"\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        inputs = self.blip_processor(image_rgb, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.blip_model.generate(**inputs, max_length=50, num_beams=5)\n",
    "            caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def detect_objects(self, image: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"Detect objects using YOLO\"\"\"\n",
    "        results = self.yolo_model(image)\n",
    "        objects = []\n",
    "        \n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    conf = box.conf[0].cpu().numpy()\n",
    "                    cls = int(box.cls[0].cpu().numpy())\n",
    "                    \n",
    "                    objects.append({\n",
    "                        'class': self.yolo_model.names[cls],\n",
    "                        'confidence': float(conf),\n",
    "                        'bbox': [float(x1), float(y1), float(x2), float(y2)],\n",
    "                        'center': [float((x1+x2)/2), float((y1+y2)/2)]\n",
    "                    })\n",
    "        \n",
    "        return objects\n",
    "    \n",
    "    def extract_poses(self, image: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"Extract human poses using MediaPipe\"\"\"\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.pose.process(image_rgb)\n",
    "        \n",
    "        poses = []\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                landmarks.append({\n",
    "                    'x': landmark.x,\n",
    "                    'y': landmark.y,\n",
    "                    'z': landmark.z,\n",
    "                    'visibility': landmark.visibility\n",
    "                })\n",
    "            poses.append({'landmarks': landmarks})\n",
    "        \n",
    "        return poses\n",
    "    \n",
    "    def extract_ocr_text(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Extract text from image using OCR\"\"\"\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            return text.strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_embeddings(self, image: np.ndarray, text: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate CLIP embeddings for image and text\"\"\"\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = self.clip_processor(\n",
    "            text=[text] if text else [\"\"],\n",
    "            images=image_rgb,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_model(**inputs)\n",
    "            image_embedding = outputs.image_embeds.cpu().numpy()[0]\n",
    "            text_embedding = outputs.text_embeds.cpu().numpy()[0]\n",
    "        \n",
    "        return image_embedding, text_embedding\n",
    "    \n",
    "    def extract_audio_text(self, video_path: str) -> str:\n",
    "        \"\"\"Extract and transcribe audio from video\"\"\"\n",
    "        try:\n",
    "            result = self.whisper_model.transcribe(video_path)\n",
    "            return result[\"text\"]\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "class VideoClustering:\n",
    "    def __init__(self, n_clusters: int = 10):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    \n",
    "    def cluster_videos(self, video_data: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Cluster videos based on content features\"\"\"\n",
    "        # Extract features for clustering\n",
    "        captions = []\n",
    "        embeddings = []\n",
    "        \n",
    "        for video in video_data:\n",
    "            # Combine all captions from keyframes\n",
    "            all_captions = \" \".join([kf['caption'] for kf in video['keyframes']])\n",
    "            captions.append(all_captions)\n",
    "            \n",
    "            # Average embeddings from all keyframes\n",
    "            if video['keyframes']:\n",
    "                avg_embedding = np.mean([kf['scene_embedding'] for kf in video['keyframes']], axis=0)\n",
    "                embeddings.append(avg_embedding)\n",
    "            else:\n",
    "                embeddings.append(np.zeros(512))  # Default CLIP embedding size\n",
    "        \n",
    "        # TF-IDF features from captions\n",
    "        tfidf_features = self.tfidf.fit_transform(captions).toarray()\n",
    "        \n",
    "        # Combine TF-IDF and visual embeddings\n",
    "        embeddings = np.array(embeddings)\n",
    "        combined_features = np.hstack([tfidf_features, embeddings])\n",
    "        \n",
    "        # Perform clustering\n",
    "        clusters = self.kmeans.fit_predict(combined_features)\n",
    "        \n",
    "        # Create mapping\n",
    "        cluster_mapping = {}\n",
    "        for i, video in enumerate(video_data):\n",
    "            cluster_mapping[video['video_id']] = int(clusters[i])\n",
    "        \n",
    "        return cluster_mapping\n",
    "    \n",
    "    def get_cluster_topics(self, video_data: List[Dict], cluster_mapping: Dict[str, int]) -> Dict[int, str]:\n",
    "        \"\"\"Extract topic keywords for each cluster\"\"\"\n",
    "        cluster_topics = {}\n",
    "        \n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            cluster_captions = []\n",
    "            for video in video_data:\n",
    "                if cluster_mapping[video['video_id']] == cluster_id:\n",
    "                    all_captions = \" \".join([kf['caption'] for kf in video['keyframes']])\n",
    "                    cluster_captions.append(all_captions)\n",
    "            \n",
    "            if cluster_captions:\n",
    "                # Get top terms for this cluster\n",
    "                cluster_text = \" \".join(cluster_captions)\n",
    "                tfidf_matrix = self.tfidf.transform([cluster_text])\n",
    "                feature_names = self.tfidf.get_feature_names_out()\n",
    "                scores = tfidf_matrix.toarray()[0]\n",
    "                \n",
    "                # Get top 5 terms\n",
    "                top_indices = np.argsort(scores)[-5:][::-1]\n",
    "                top_terms = [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "                cluster_topics[cluster_id] = \", \".join(top_terms)\n",
    "            else:\n",
    "                cluster_topics[cluster_id] = \"empty\"\n",
    "        \n",
    "        return cluster_topics\n",
    "\n",
    "class QdrantManager:\n",
    "    def __init__(self, host: str = \"localhost\", port: int = 6333):\n",
    "        self.client = qdrant_client.QdrantClient(host=host, port=port)\n",
    "        self.collection_name = \"video_keyframes\"\n",
    "    \n",
    "    def create_collection(self):\n",
    "        \"\"\"Create Qdrant collection for video keyframes\"\"\"\n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config={\n",
    "                    \"image\": VectorParams(size=512, distance=Distance.COSINE),\n",
    "                    \"text\": VectorParams(size=512, distance=Distance.COSINE),\n",
    "                    \"pose\": VectorParams(size=64, distance=Distance.COSINE),\n",
    "                }\n",
    "            )\n",
    "            logger.info(f\"Created collection: {self.collection_name}\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Collection might already exist: {e}\")\n",
    "    \n",
    "    def index_keyframe(self, keyframe: KeyframeData):\n",
    "        \"\"\"Index a single keyframe in Qdrant\"\"\"\n",
    "        # Create pose embedding (simplified - you might want more sophisticated pose encoding)\n",
    "        pose_embedding = np.zeros(64)\n",
    "        if keyframe.poses:\n",
    "            # Simple pose encoding - flatten first few landmarks\n",
    "            landmarks = keyframe.poses[0]['landmarks'][:16]  # First 16 landmarks\n",
    "            for i, lm in enumerate(landmarks):\n",
    "                if i*4 < 64:\n",
    "                    pose_embedding[i*4:i*4+4] = [lm['x'], lm['y'], lm['z'], lm['visibility']]\n",
    "        \n",
    "        point = PointStruct(\n",
    "            id=hashlib.md5(keyframe.frame_id.encode()).hexdigest(),\n",
    "            vector={\n",
    "                \"image\": keyframe.scene_embedding.tolist(),\n",
    "                \"text\": keyframe.text_embedding.tolist(),\n",
    "                \"pose\": pose_embedding.tolist()\n",
    "            },\n",
    "            payload={\n",
    "                \"video_id\": keyframe.video_id,\n",
    "                \"timestamp\": keyframe.timestamp,\n",
    "                \"image_path\": keyframe.image_path,\n",
    "                \"caption\": keyframe.caption,\n",
    "                \"objects\": keyframe.objects,\n",
    "                \"poses\": keyframe.poses,\n",
    "                \"actions\": keyframe.actions,\n",
    "                \"ocr_text\": keyframe.ocr_text\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=[point]\n",
    "        )\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, storage_manager: GoogleStorageManager, analyzer: MultiModalAnalyzer, \n",
    "                 qdrant_manager: QdrantManager):\n",
    "        self.storage = storage_manager\n",
    "        self.analyzer = analyzer\n",
    "        self.qdrant = qdrant_manager\n",
    "        self.clustering = VideoClustering()\n",
    "    \n",
    "    async def process_video(self, video_path: str, video_id: str) -> VideoMetadata:\n",
    "        \"\"\"Process a single video through the entire pipeline\"\"\"\n",
    "        logger.info(f\"Processing video: {video_id}\")\n",
    "        \n",
    "        # Extract video metadata\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "        \n",
    "        # Extract keyframes\n",
    "        keyframes = self.analyzer.extract_keyframes(video_path)\n",
    "        logger.info(f\"Extracted {len(keyframes)} keyframes\")\n",
    "        \n",
    "        # Extract audio transcription\n",
    "        audio_text = self.analyzer.extract_audio_text(video_path)\n",
    "        \n",
    "        # Process each keyframe\n",
    "        processed_keyframes = []\n",
    "        for i, (timestamp, frame) in enumerate(keyframes):\n",
    "            frame_id = f\"{video_id}_frame_{i:04d}\"\n",
    "            \n",
    "            # Save keyframe image\n",
    "            keyframe_path = f\"keyframes/{video_id}/{frame_id}.jpg\"\n",
    "            local_keyframe_path = f\"/tmp/{frame_id}.jpg\"\n",
    "            cv2.imwrite(local_keyframe_path, frame)\n",
    "            \n",
    "            # Upload to storage\n",
    "            remote_path = self.storage.upload_file(local_keyframe_path, keyframe_path)\n",
    "            \n",
    "            # Analyze keyframe\n",
    "            caption = self.analyzer.generate_caption(frame)\n",
    "            objects = self.analyzer.detect_objects(frame)\n",
    "            poses = self.analyzer.extract_poses(frame)\n",
    "            ocr_text = self.analyzer.extract_ocr_text(frame)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            combined_text = f\"{caption} {ocr_text} {audio_text}\"\n",
    "            scene_embedding, text_embedding = self.analyzer.generate_embeddings(frame, combined_text)\n",
    "            \n",
    "            # Create keyframe data\n",
    "            keyframe_data = KeyframeData(\n",
    "                frame_id=frame_id,\n",
    "                video_id=video_id,\n",
    "                timestamp=timestamp,\n",
    "                image_path=remote_path,\n",
    "                caption=caption,\n",
    "                objects=objects,\n",
    "                poses=poses,\n",
    "                actions=[],  # TODO: Add action recognition\n",
    "                ocr_text=ocr_text,\n",
    "                scene_embedding=scene_embedding,\n",
    "                text_embedding=text_embedding,\n",
    "                pose_embedding=np.zeros(64)  # Simplified\n",
    "            )\n",
    "            \n",
    "            # Index in Qdrant\n",
    "            self.qdrant.index_keyframe(keyframe_data)\n",
    "            processed_keyframes.append(keyframe_data)\n",
    "        \n",
    "        return VideoMetadata(\n",
    "            video_id=video_id,\n",
    "            filename=Path(video_path).name,\n",
    "            duration=duration,\n",
    "            fps=fps,\n",
    "            resolution=(width, height)\n",
    "        )\n",
    "    \n",
    "    async def process_batch(self, video_paths: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple videos and perform clustering\"\"\"\n",
    "        processed_videos = []\n",
    "        \n",
    "        for video_path in video_paths:\n",
    "            video_id = Path(video_path).stem\n",
    "            metadata = await self.process_video(video_path, video_id)\n",
    "            processed_videos.append(metadata)\n",
    "        \n",
    "        # TODO: Perform clustering after processing all videos\n",
    "        # This would require collecting all keyframe data first\n",
    "        \n",
    "        return {\n",
    "            \"processed_count\": len(processed_videos),\n",
    "            \"videos\": processed_videos\n",
    "        }\n",
    "\n",
    "# Main processing function\n",
    "async def main():\n",
    "    # Initialize components\n",
    "    storage_manager = GoogleStorageManager(\"your-bucket-name\")\n",
    "    analyzer = MultiModalAnalyzer()\n",
    "    qdrant_manager = QdrantManager()\n",
    "    \n",
    "    # Create collection\n",
    "    qdrant_manager.create_collection()\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = VideoProcessor(storage_manager, analyzer, qdrant_manager)\n",
    "    \n",
    "    # Get video list from storage\n",
    "    video_files = storage_manager.list_files(\"Videos_L21_a/\")\n",
    "    video_paths = [f\"gs://your-bucket/{file}\" for file in video_files if file.endswith('.mp4')]\n",
    "    \n",
    "    # Process videos\n",
    "    results = await processor.process_batch(video_paths[:5])  # Process first 5 videos\n",
    "    \n",
    "    logger.info(f\"Processing complete: {results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
